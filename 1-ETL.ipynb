{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a39f84f-74fe-48db-98e0-0cd26e4e103f",
   "metadata": {},
   "source": [
    "# Extract Transform Load\n",
    "In this notebook, we extract data from the praise worksheet, transform it into suitable, clean data, and load it into the outputs directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b35d35c8-fa43-4022-9079-89b29cc10d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849607d6-9d9a-4dc4-853c-fca90c45bf2c",
   "metadata": {},
   "source": [
    "## Total Hours Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1c1e6558-d5da-4c95-9a5f-a0344fd3943e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Handle</th>\n",
       "      <th>Impact Hours</th>\n",
       "      <th>Payment Status</th>\n",
       "      <th>Estimate IH if nothing was deducted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>zeptimusq</td>\n",
       "      <td>669.466400</td>\n",
       "      <td>unpaid</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>santigs67</td>\n",
       "      <td>593.609903</td>\n",
       "      <td>unpaid</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cranders71</td>\n",
       "      <td>394.858600</td>\n",
       "      <td>unpaid</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ygganderson</td>\n",
       "      <td>371.543022</td>\n",
       "      <td>Partial paid</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sembrestels</td>\n",
       "      <td>361.269427</td>\n",
       "      <td>Partial paid</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>solsista</td>\n",
       "      <td>290.377053</td>\n",
       "      <td>unpaid</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>iviangita</td>\n",
       "      <td>254.522356</td>\n",
       "      <td>Partial paid</td>\n",
       "      <td>509.044711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>akrtws</td>\n",
       "      <td>247.108503</td>\n",
       "      <td>unpaid</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>mateodaza</td>\n",
       "      <td>224.992460</td>\n",
       "      <td>Partial paid</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>metaverde</td>\n",
       "      <td>204.586181</td>\n",
       "      <td>unpaid</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>durgadasji</td>\n",
       "      <td>201.696127</td>\n",
       "      <td>unpaid</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>juankbell</td>\n",
       "      <td>165.431131</td>\n",
       "      <td>Partial paid</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>jessicazartler</td>\n",
       "      <td>163.611904</td>\n",
       "      <td>Partial paid</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>griffgreen</td>\n",
       "      <td>148.821213</td>\n",
       "      <td>paid</td>\n",
       "      <td>992.141418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>mzargham</td>\n",
       "      <td>148.699578</td>\n",
       "      <td>unpaid</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Handle  Impact Hours Payment Status  \\\n",
       "0        zeptimusq    669.466400         unpaid   \n",
       "1        santigs67    593.609903         unpaid   \n",
       "2       cranders71    394.858600         unpaid   \n",
       "3      ygganderson    371.543022   Partial paid   \n",
       "4      sembrestels    361.269427   Partial paid   \n",
       "5         solsista    290.377053         unpaid   \n",
       "6        iviangita    254.522356   Partial paid   \n",
       "7           akrtws    247.108503         unpaid   \n",
       "8        mateodaza    224.992460   Partial paid   \n",
       "9        metaverde    204.586181         unpaid   \n",
       "10      durgadasji    201.696127         unpaid   \n",
       "11       juankbell    165.431131   Partial paid   \n",
       "12  jessicazartler    163.611904   Partial paid   \n",
       "13      griffgreen    148.821213           paid   \n",
       "14        mzargham    148.699578         unpaid   \n",
       "\n",
       "    Estimate IH if nothing was deducted  \n",
       "0                                   NaN  \n",
       "1                                   NaN  \n",
       "2                                   NaN  \n",
       "3                                   NaN  \n",
       "4                                   NaN  \n",
       "5                                   NaN  \n",
       "6                            509.044711  \n",
       "7                                   NaN  \n",
       "8                                   NaN  \n",
       "9                                   NaN  \n",
       "10                                  NaN  \n",
       "11                                  NaN  \n",
       "12                                  NaN  \n",
       "13                           992.141418  \n",
       "14                                  NaN  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_hours = pd.read_excel(\"data/TEC Praise Quantification.xlsx\", sheet_name=\"Total Impact Hours so far\", engine='openpyxl', header=1, index_col=0, usecols=\"A:D\").dropna(thresh=2).reset_index()\n",
    "total_hours.to_csv('outputs/total_hours.csv', index=False)\n",
    "total_hours.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e81b5b4-35f7-404c-9c0f-afbb3297c8b0",
   "metadata": {},
   "source": [
    "## Praise Dataset\n",
    "We consider three batches of praise. Batch one is October 2020, and December2020-May2021. Batch two is prior to December 2020 (other than October). Batch three is after May 7th 2021. Batch one is the original batch that all analysis was performed on. Chronologically, prior to Batch one (batch two) is the early days of praise, and batch 3 is praise that started after the initial praise analysis began."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c664246b-6223-4271-abfa-ace7b902cfe9",
   "metadata": {},
   "source": [
    "#### Batch One"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd7e4e9c-c66c-4f13-92a4-d8d2e57a1a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "periods = [\n",
    "    \"#17 May 7\",\n",
    "    \"#16 Apr 24\",\n",
    "    \"#15 Apr 9\",\n",
    "    \"#14 Mar 26\",\n",
    "    \"#13 Mar 12\",\n",
    "    \"#12 Feb 26\",\n",
    "    \"#11 Feb 12\",\n",
    "    \"#10 Jan 29\",\n",
    "    \"#9 Jan 15\", \n",
    "    \"#8 Jan 1\",\n",
    "    \"#7 Dec 18\",\n",
    "    \"#6 Dec 4\",\n",
    "    \"#2 Oct 9\",\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ed32d96-c145-4f57-a84f-bdb8d89b0cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_period_from_sheet(period: str, batch: str) -> pd.DataFrame:\n",
    "        # Load the data\n",
    "        df = pd.read_excel('data/TEC Praise Quantification.xlsx', skiprows=2, sheet_name=period,engine='openpyxl', usecols=\"A:M\")\n",
    "        \n",
    "        # Add a period column\n",
    "        df['period'] = period\n",
    "        df['batch'] = batch\n",
    "        \n",
    "        # Remove the validator normalization as it is confusing and unecessary for analysis\n",
    "        df.columns = list(df.columns[:6]) + ['v1 norm', 'v2 norm', 'v3 norm'] + list(df.columns[9:])\n",
    "        df = df.dropna(thresh=8).drop(['v1 norm', 'v2 norm', 'v3 norm'], axis=1)\n",
    "        \n",
    "        # Return the loaded df\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac21b5fa-747b-46ac-bb5b-b4ef7f0e3852",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and concatenate data\n",
    "data = pd.concat([read_period_from_sheet(p, batch='Batch 1') for p in periods])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8fc35ed3-4743-41fe-b8f3-375e56a1ae9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine these duplicate columns and drop the lesser named one.\n",
    "df = data.copy()\n",
    "# To\n",
    "df['To'] = df['To'].combine_first(df['To.1']).combine_first(df['Unnamed: 12'])\n",
    "df = df.drop(['To.1', 'Unnamed: 12'], axis=1)\n",
    "\n",
    "# IH Per Praise\n",
    "df['IH per Praise'] = df['IH per Praise'].combine_first(df['Cred per Praise'])\n",
    "df = df.drop('Cred per Praise', axis=1)\n",
    "\n",
    "# IH Per Person Per Period\n",
    "df['IH per person'] = df['IH per person'].combine_first(df['Cred per person'])\n",
    "df = df.drop('Cred per person', axis=1)\n",
    "\n",
    "# Rename The Institution Column\n",
    "df = df.rename({'Unnamed: 3':'Institution'}, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6add775d-72a3-4af1-beab-da6942402ca2",
   "metadata": {},
   "source": [
    "#### Batch Two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9cd7a0f-01aa-42d1-b5fd-aeb43b203c9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>To</th>\n",
       "      <th>From</th>\n",
       "      <th>Reason for dishing</th>\n",
       "      <th>Institution</th>\n",
       "      <th>Date</th>\n",
       "      <th>Room</th>\n",
       "      <th>Avg %</th>\n",
       "      <th>IH per Praise</th>\n",
       "      <th>IH per person</th>\n",
       "      <th>period</th>\n",
       "      <th>batch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>VitorMarthendal</td>\n",
       "      <td>iviangita#3204</td>\n",
       "      <td>for pushing to get the Hatch Dashboard out</td>\n",
       "      <td>Token Engineering Commons</td>\n",
       "      <td>May-07-2021</td>\n",
       "      <td>üôèpraise</td>\n",
       "      <td>0.022117</td>\n",
       "      <td>26.540405</td>\n",
       "      <td>7.559265</td>\n",
       "      <td>#17 May 7</td>\n",
       "      <td>Batch 1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                To            From  \\\n",
       "0  VitorMarthendal  iviangita#3204   \n",
       "\n",
       "                            Reason for dishing                Institution  \\\n",
       "0  for pushing to get the Hatch Dashboard out   Token Engineering Commons   \n",
       "\n",
       "          Date     Room     Avg %  IH per Praise  IH per person     period  \\\n",
       "0  May-07-2021  üôèpraise  0.022117      26.540405       7.559265  #17 May 7   \n",
       "\n",
       "     batch  \n",
       "0  Batch 1  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5c53be68-b6a3-4f33-b423-c9d256b2b035",
   "metadata": {},
   "outputs": [],
   "source": [
    "periods = [\n",
    "    \"#5 Nov 20\", \n",
    "    \"#4 Nov 6\", \n",
    "    \"#3 Oct 23\", \n",
    "    \"#1 Sept 24\", \n",
    "    \"#0 Sept 7 (historic)\", \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "95b83fe8-fd71-4835-8c57-de2b6f6a4c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = pd.concat([read_period_from_sheet(p, batch='Batch 2') for p in periods])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "200493db-3b75-4d39-8b19-0d8bd7bda20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine these duplicate columns and drop the lesser named one.\n",
    "df2 = data2.copy()\n",
    "# To\n",
    "df2['To'] = df2['To'].combine_first(df2['To.1'])\n",
    "df2 = df2.drop(['To.1'], axis=1)\n",
    "\n",
    "# IH Per Praise\n",
    "df2 = df2.rename({'Cred per Praise':'IH per Praise'}, axis=1)\n",
    "\n",
    "# IH Per Person Per Period\n",
    "df2 = df2.rename({'Cred per person':'IH per person'}, axis=1)\n",
    "\n",
    "# Rename The Institution Column\n",
    "df2 = df2.rename({'Unnamed: 3':'Institution'}, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f60687e-0fde-4ee4-8687-6efbd590ce0d",
   "metadata": {},
   "source": [
    "#### Batch 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6322da72-3a51-405d-9253-8d85ab73651f",
   "metadata": {},
   "outputs": [],
   "source": [
    "periods = [\n",
    "    \"#18 May 21\",\n",
    "    \"#19 Jun 4\",\n",
    "    \"#20 Jun 18\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ef479abd-5b83-4350-a490-b6fb60de1fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data3 = pd.concat([read_period_from_sheet(p, batch='Batch 3') for p in periods])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8db1c589-67d2-45ab-8c1a-34030d685c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine these duplicate columns and drop the lesser named one.\n",
    "df3 = data3.copy()\n",
    "# To\n",
    "df3['To'] = df3['To'].combine_first(df3['Unnamed: 12'])\n",
    "df3 = df3.drop(['Unnamed: 12'], axis=1)\n",
    "\n",
    "# Rename The Institution Column\n",
    "df3 = df3.rename({'Unnamed: 3':'Institution'}, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6da23a2-b449-49f3-bcba-c2011ce32f23",
   "metadata": {},
   "source": [
    "### Concatenate Praise Batches into Praise Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "aea094ef-3b0e-49eb-baf9-6ffce7fffaad",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([df, df2, df3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6d8e72-a52a-4eeb-babd-535e584731d0",
   "metadata": {},
   "source": [
    "### Resolving Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d9ae45d7-5d15-43b8-9d0e-7d4dde9dc971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are still missing 11 names\n"
     ]
    }
   ],
   "source": [
    "names_df = pd.read_excel('data/TEC Praise Quantification.xlsx',sheet_name=\"DO NOT TOUCH Imported\",engine='openpyxl', usecols=\"A:D\")\n",
    "\n",
    "#### We create a dictionary that matches each non-null entry in the spreadsheet to its \"IH & CSTK Handle\"\n",
    "source_cols = names_df.columns\n",
    "names_dict = {} #create a blank dictionary\n",
    "\n",
    "for i in range(len(names_df)):\n",
    "    for col in source_cols:\n",
    "        name_to_consider = names_df.loc[i, col]\n",
    "        canonical_name = names_df.loc[i,\"IH & CSTK Handle\"]\n",
    "        if pd.isna(canonical_name):\n",
    "            canonical_name = name_to_consider\n",
    "        if not(pd.isna(name_to_consider)):\n",
    "            names_dict[name_to_consider] = canonical_name\n",
    "\n",
    "#### Did we catch them all? See if there's anything in the combned user sets of \"To\" and \"From\" that isn't a key in the names_dict dictionary. \n",
    "combined_users = set([]).union(set(receivers[\"From\"]),set(receivers[\"To\"]))\n",
    "names_uncaught = sorted([str(x) for x in combined_users if (not(x in names_dict.keys()) and not(pd.isna(x)))])\n",
    "\n",
    "# print(\"All told, there are {} names in our data set that lack canonical representations: \\n\".format(str(len(names_uncaught))))\n",
    "# print(\"We do not have canonical representations for the following names: \\n\")\n",
    "# for name in names_uncaught:\n",
    "#     print(str(name))\n",
    "\n",
    "def clean_name(name):\n",
    "    new_name = str(name)\n",
    "    new_name = new_name.lower()\n",
    "    new_name = re.sub('#\\d\\d\\d\\d','',new_name)\n",
    "    new_name = re.sub('[^A-Za-z0-9]+', '', new_name) #remove all non-alphanumeric characters\n",
    "    return new_name\n",
    "\n",
    "clean_name(\"zeptimusQ\")\n",
    "clean_name(\"ygg_anderson\")\n",
    "clean_name(\"AmwFund#0979\")\n",
    "clean_name(\"Caeser (PST)#0046\")\n",
    "\n",
    "cleaned_keys = []\n",
    "original_keys = list(names_dict.keys())\n",
    "for key in original_keys:\n",
    "    clean_key = clean_name(key)\n",
    "    cleaned_keys.append(clean_key)\n",
    "    if not(clean_key == key):\n",
    "        names_dict[clean_key] = names_dict[key]\n",
    "    \n",
    "new_pairs = {}\n",
    "\n",
    "for name in names_uncaught:\n",
    "    clean_version = clean_name(name) #clean the name \n",
    "    key_matches = [key for key in cleaned_keys if clean_version == key] #make a list of all keys with same cleaned name\n",
    "    if len(key_matches) > 0:\n",
    "        for match in key_matches:\n",
    "            if match in names_dict.keys():\n",
    "                right_name = names_dict[match]\n",
    "                new_pairs[name] = right_name\n",
    "                names_dict[name] = right_name\n",
    "                break\n",
    "                    \n",
    "# print(\"We were able to add the following names: \\n\")\n",
    "# for name in new_pairs.keys():\n",
    "#     print(str(name) + \"\\t ----> \\t\" + str(names_dict[name]))\n",
    "\n",
    "still_uncaught_names = sorted([str(x) for x in combined_users if (not(x in names_dict.keys()))])\n",
    "num_still_uncaught_names = str(len(still_uncaught_names))\n",
    "print(\"We are still missing {} names\".format(num_still_uncaught_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2b1326cf-ddaf-4d9a-b150-f3bf21673356",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 0 remaining users with no representation.\n",
      "These users are \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def total_appearances(name, data):\n",
    "    from_appearances = sum(data[\"From\"] == name)\n",
    "    to_appearances = sum(data[\"To\"] == name)\n",
    "    total = from_appearances + to_appearances\n",
    "    return total \n",
    "\n",
    "still_uncaught_appearances = [total_appearances(x, data) for x in still_uncaught_names]\n",
    "\n",
    "uncaught_df = pd.DataFrame({\"name\": still_uncaught_names, \"appearances\" : still_uncaught_appearances})\n",
    "\n",
    "for name in still_uncaught_names:\n",
    "    names_dict[name] = name\n",
    "\n",
    "last_remaining = [user for user in combined_users if(not(user in names_dict.keys()))]\n",
    "num_remaining = len(last_remaining)\n",
    "print(\"There are {} remaining users with no representation.\".format(num_remaining))\n",
    "print(\"These users are \\n\")\n",
    "\n",
    "for name in last_remaining:\n",
    "    print(name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "d512c86a-e41f-4912-b7f7-8e593a0de4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[:,\"To\"] = data.loc[:,\"To\"].map(names_dict)\n",
    "data.loc[:, \"From\"] = data.loc[:,\"From\"].map(names_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34df313c-2054-4e89-b9d8-e62ec05039d3",
   "metadata": {},
   "source": [
    "### Save Cleaned Praise Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "4ea347c4-50df-4948-9fc6-4b7489ee3f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('outputs/praise_data.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99181faf-d519-4e9c-b9a7-0fa0e87eb799",
   "metadata": {},
   "source": [
    "### Split Receivers and Quantifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "4f07a885-61f1-4fcf-8476-361472d9f606",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "receivers = data[(~data[['Institution', 'Date', 'Room']].isna().all(axis=1)) & (data['From'] != 'Quantifiers')]\n",
    "quantifiers = data[(data[['Institution', 'Date', 'Room']].isna().all(axis=1)) | (data['From'] == 'Quantifiers')]\n",
    "len(receivers) + len(quantifiers) == len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "83fcc927-4ca1-4f0a-a3af-3450bf3928f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "receivers.to_csv('outputs/receivers.csv', index=False)\n",
    "quantifiers.to_csv('outputs/quantifiers.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0942ca34-01ad-4814-831a-9e6c62c464c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
